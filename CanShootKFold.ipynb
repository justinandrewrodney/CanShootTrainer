{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed745a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "#tf.random.set_seed(1)\n",
    "\n",
    "def get_feat_and_labels(filename):\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    data[\"Angle\"] = np.abs(np.arctan2(data[\"OppY\"], data[\"OppX\"]))\n",
    "    data[\"OppY\"] = np.abs(data[\"OppY\"])\n",
    "    data = data[[\"Angle\",  \"DistanceToGoal\", \"DistanceToOpp\",  \"OppX\", \"OppY\",\"Success\"]]\n",
    "    \n",
    "    #Converting text yes no to int.\n",
    "    if(data[\"Success\"].dtype == object):\n",
    "        data[\"Success\"] = (data[\"Success\"] == \"YES\")*1\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaf86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printYesNoCount(data):\n",
    "    if type(data) == pd.DataFrame:\n",
    "        success_no = data[data['Success'] == 0]\n",
    "        success_yes = data[data['Success'] == 1]\n",
    "    else:\n",
    "        success_no = data[data == 0]\n",
    "        success_yes = data[data == 1]\n",
    "    print('success no:', success_no.shape[0])\n",
    "    print('success yes:', success_yes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf458a18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Code used to create training data\"\"\"\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "def get_data(names):\n",
    "    original_data = get_feat_and_labels(names[0])\n",
    "    \n",
    "    for n in names[1:]:\n",
    "        d = get_feat_and_labels(n)        \n",
    "        original_data = pd.concat([original_data, d],axis=0)\n",
    "    \n",
    "    #printYesNoCount(original_data)\n",
    "    return original_data\n",
    "\n",
    "# data_names = [\n",
    "#                 \"..\\MotionTestingData\\CanShootTolLocal2x.csv\",\n",
    "#                 \"..\\MotionTestingData\\CanShootTolLocal3x.csv\",\n",
    "#                 \"..\\MotionTestingData\\CanShootTolLocal4x.csv\",\n",
    "#                 \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2MotionTol2x.csv\",\n",
    "#                 \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2MotionTol3x.csv\",\n",
    "#                 \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2MotionTol4x.csv\",\n",
    "#                 \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2plus75tolerance.csv\",\n",
    "#                 \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2Tol3x.csv\",\n",
    "#                 \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2Tol4x.csv\",\n",
    "\n",
    "#                 \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2Tolerance2x.csv\",\n",
    "#                 \"..\\\\NewGoodData\\Originals\\CanShoot2Behavior.csv\",\n",
    "#                 \"..\\\\NewGoodData\\Originals\\CanShoot2Bottom.csv\",\n",
    "#                 \"..\\\\NewGoodData\\Originals\\CanShoot2Upper.csv\",\n",
    "#                 \"..\\\\NewGoodData\\Originals\\CanShoot2Mid.csv\"\n",
    "    \n",
    "#              ]\n",
    "data_names = [\n",
    "               \"..\\MotionTestingData\\CanShootStillLocal3x.csv\",\n",
    "                \n",
    "                \"..\\MotionTestingData\\CanShootTolLocal2x.csv\",\n",
    "                \"..\\MotionTestingData\\CanShootTolLocal3x.csv\",\n",
    "                \"..\\MotionTestingData\\CanShootTolLocal4x.csv\",\n",
    "                \n",
    "                \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2MotionTol2x.csv\",\n",
    "                \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2MotionTol3x.csv\",\n",
    "                \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2MotionTol4x.csv\",\n",
    "                \n",
    "                \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2plus75tolerance.csv\",\n",
    "                \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2Tol3x.csv\",\n",
    "                \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2Tol4x.csv\",\n",
    "                \"..\\\\NewGoodData\\Testfeb8num3\\CanShoot2Tolerance2x.csv\",\n",
    "                \n",
    "                \"..\\\\NewGoodData\\Originals\\CanShoot2Behavior.csv\",\n",
    "                \n",
    "                \"..\\\\NewGoodData\\Originals\\CanShoot2Bottom.csv\",\n",
    "                \"..\\\\NewGoodData\\Originals\\CanShoot2Upper.csv\",\n",
    "                \"..\\\\NewGoodData\\Originals\\CanShoot2Center.csv\",\n",
    "                \"..\\\\NewGoodData\\Originals\\CanShoot2Mid.csv\"\n",
    "\n",
    "             ]\n",
    "\n",
    "    \n",
    "#Load all data  normally the stratified train/valid split would be used created.\n",
    "train_data = get_data(data_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eeac1f",
   "metadata": {},
   "source": [
    "# Train Model loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab88b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(num_feats):\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.BatchNormalization(input_dim=num_feats),\n",
    "      tf.keras.layers.Dense(6, activation='relu'),\n",
    "\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.Dense(3, activation='swish'),\n",
    "\n",
    "      tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.05),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Recall(), tf.keras.metrics.Precision()], \n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "loud_mode = False\n",
    "def train_model(model, inputs, targets):\n",
    "    #Model early stops on a validation loss on a split of the training data\n",
    "#     print(inputs.shape)\n",
    "    \n",
    "#     f_train, f_valid, l_train, l_valid = train_test_split(inputs, targets, test_size=0.33, stratify=targets)#, random_state=3, stratify=targets)\n",
    "    \n",
    "#     from keras.callbacks import ReduceLROnPlateau\n",
    "#     reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "#                               patience=75, min_lr=0.001)\n",
    "#     model.fit(\n",
    "#         f_train,\n",
    "#         l_train,\n",
    "#         batch_size=16,\n",
    "#         epochs = 64,   \n",
    "#         validation_data= (f_valid, l_valid ),\n",
    "#         callbacks=[reduce_lr],\n",
    "#         verbose = loud_mode\n",
    "#     )\n",
    "\n",
    "\n",
    "#     #add early stopping\n",
    "#     early_stop= tf.keras.callbacks.EarlyStopping(\n",
    "#         monitor='val_loss', min_delta=0, patience=50, verbose=0,\n",
    "#         mode='min', baseline=None, restore_best_weights=True\n",
    "#     )\n",
    "\n",
    "#     model.fit(\n",
    "#         f_train,\n",
    "#         l_train,\n",
    "#         batch_size=16,\n",
    "#         epochs = 512,   \n",
    "#         validation_data= (f_valid, l_valid ),\n",
    "#         callbacks=[reduce_lr, early_stop] ,\n",
    "#         verbose = loud_mode\n",
    "#     )\n",
    "\n",
    "\n",
    "#     inputs = tf.convert_to_tensor(inputs,dtype=tf.float32)\n",
    "#     targets= tf.convert_to_tensor(targets,dtype=tf.float32)\n",
    "#     print(inputs.shape)\n",
    "#     print(targets.shape)\n",
    "\n",
    "    \n",
    "    #Reduce lr on loss instead of val_los\n",
    "    from keras.callbacks import ReduceLROnPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                              patience=75, min_lr=0.001)\n",
    "    model.fit(\n",
    "        inputs,\n",
    "        targets,\n",
    "        batch_size=16,\n",
    "        epochs = 64,   \n",
    "        callbacks=[reduce_lr],\n",
    "        verbose = loud_mode\n",
    "    )\n",
    "\n",
    "\n",
    "    #Early stop on loss instead of val_loss.\n",
    "    early_stop= tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss', min_delta=0, patience=50, verbose=0,\n",
    "        mode='min', baseline=None, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        inputs,\n",
    "        targets,\n",
    "        batch_size=16,\n",
    "        epochs = 512,   \n",
    "        callbacks=[reduce_lr, early_stop] ,\n",
    "        verbose = loud_mode\n",
    "    )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72167d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResults(name, acc_per_fold, train_idx, test_idx):\n",
    "    output_file = open(f'CrossValidationResults\\\\{name}.csv', \"w\")\n",
    "    output_file.write('Fold,Accuracy,Train_size,Test_size'+'\\n')\n",
    "    \n",
    "    print(f'------------------------------------------------------------------------\\n{name}\\nScore per fold:')\n",
    "    for i in range(0, len(acc_per_fold)):\n",
    "        print(f'> Fold {i+1} - Accuracy: {acc_per_fold[i]}%')\n",
    "        output_file.write(f'{i+1},{acc_per_fold[i]},{len(train_idx[i])},{len(test_idx[i])}'+'\\n')\n",
    "    \n",
    "    #output_file.write(f'{np.mean(acc_per_fold)},{np.std(acc_per_fold)}'+'\\n')\n",
    "    output_file.close()   \n",
    "    \n",
    "    #print('------------------------------------------------------------------------\\nAverage scores for all folds:')\n",
    "    #print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ad2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#referenced:\n",
    "#https://medium.com/analytics-vidhya/using-the-corrected-paired-students-t-test-for-comparing-the-performance-of-machine-learning-dc6529eaa97f\n",
    "def CorrectedResampledPairedT_Test(nn_acc_per_fold, lr_acc_per_fold, trains, tests):\n",
    "    diff = [y - x for y, x in zip(nn_acc_per_fold, lr_acc_per_fold)]\n",
    "    \n",
    "    mean_diff = np.mean(diff)\n",
    "    s2 = np.var(diff, ddof=1)\n",
    "    \n",
    "    #Size of folds\n",
    "    n1 = np.mean(trains)\n",
    "    n2 = np.mean(tests)\n",
    "    print(f'n1:{n1} n2:{n2}')\n",
    "    \n",
    "    #total number of folds\n",
    "    K = len(diff)\n",
    "    S2 = (1/K + n2/n1) * s2\n",
    "    \n",
    "    m_s =  mean_diff / np.sqrt(S2)\n",
    "    \n",
    "    from scipy.stats import t\n",
    "    \n",
    "    #Compute p-value and plot the results \n",
    "    Pvalue = ((1 - t.cdf(np.abs(m_s), K-1))*2.0)\n",
    "    \n",
    "    print(m_s)\n",
    "    print(Pvalue)\n",
    "    print(\"T\", t.cdf(np.abs(m_s), K-1))\n",
    "    #print(\"2.276003475\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c046d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(542, 5)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Length of train: 487 \t length of test: 55\n",
      "NN: Score for fold 1: binary_accuracy of 83.63636136054993%; loss of 0.34699463844299316\n",
      "LR: Score for fold 1: binary_accuracy of 80.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Length of train: 487 \t length of test: 55\n",
      "NN: Score for fold 2: binary_accuracy of 87.27272748947144%; loss of 0.35817596316337585\n",
      "LR: Score for fold 2: binary_accuracy of 80.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "NN: Score for fold 3: binary_accuracy of 83.33333134651184%; loss of 0.31542453169822693\n",
      "LR: Score for fold 3: binary_accuracy of 83.33333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "NN: Score for fold 4: binary_accuracy of 83.33333134651184%; loss of 0.4435070753097534\n",
      "LR: Score for fold 4: binary_accuracy of 87.03703703703704%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x000002A281AB8A68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "NN: Score for fold 5: binary_accuracy of 87.03703880310059%; loss of 0.30506446957588196\n",
      "LR: Score for fold 5: binary_accuracy of 83.33333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002A28C8434C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "NN: Score for fold 6: binary_accuracy of 85.18518805503845%; loss of 0.32668375968933105\n",
      "LR: Score for fold 6: binary_accuracy of 87.03703703703704%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "NN: Score for fold 7: binary_accuracy of 90.74074029922485%; loss of 0.27301204204559326\n",
      "LR: Score for fold 7: binary_accuracy of 88.88888888888889%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "NN: Score for fold 8: binary_accuracy of 87.03703880310059%; loss of 0.3114638030529022\n",
      "LR: Score for fold 8: binary_accuracy of 87.03703703703704%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "NN: Score for fold 9: binary_accuracy of 85.18518805503845%; loss of 0.3896179497241974\n",
      "LR: Score for fold 9: binary_accuracy of 83.33333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "NN: Score for fold 10: binary_accuracy of 77.77777910232544%; loss of 0.37274590134620667\n",
      "LR: Score for fold 10: binary_accuracy of 83.33333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Length of train: 487 \t length of test: 55\n",
      "NN: Score for fold 11: binary_accuracy of 80.0000011920929%; loss of 0.3767249286174774\n",
      "LR: Score for fold 11: binary_accuracy of 81.81818181818183%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Length of train: 487 \t length of test: 55\n",
      "NN: Score for fold 12: binary_accuracy of 81.81818127632141%; loss of 0.35195186734199524\n",
      "LR: Score for fold 12: binary_accuracy of 90.9090909090909%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "NN: Score for fold 13: binary_accuracy of 81.4814805984497%; loss of 0.31499725580215454\n",
      "LR: Score for fold 13: binary_accuracy of 81.48148148148148%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Length of train: 488 \t length of test: 54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "#based on: \n",
    "# https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-keras.md\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def ten_ten_fold_cross_val():\n",
    "    # Combine all of training data for cross_validation\n",
    "    fold_info = []\n",
    "    train_inf = []\n",
    "    test_inf = []\n",
    "    \n",
    "    nn_acc_per_fold= []    \n",
    "    lr_acc_per_fold= []\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    fold_no = 1\n",
    "    \n",
    "    inputs = train_data.drop(columns=\"Success\")\n",
    "    targets = train_data[\"Success\"]\n",
    "\n",
    "    print(inputs.shape)\n",
    "\n",
    "    #kfold = KFold(n_splits=10, shuffle=True)\n",
    "    kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=10)#,random_state=0)\n",
    "        \n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "        print(f'------------------------------------------------------------------------\\nTraining for fold {fold_no} ...')\n",
    "        #Store train test split\n",
    "        fold_info.append([train,test])\n",
    "        train_inf.append(train)\n",
    "        test_inf.append(test)\n",
    "        print(f'Length of train: {len(train)} \\t length of test: {len(test)}')\n",
    "\n",
    "        # -- Neural Network --\n",
    "        tf.keras.backend.clear_session()\n",
    "        cv_nn_model = get_model(inputs.shape[1])\n",
    "\n",
    "        #Train model using train split.\n",
    "        train_model(cv_nn_model, inputs.iloc[train], targets.iloc[train])\n",
    "\n",
    "        # -- Test --\n",
    "        nn_scores = cv_nn_model.evaluate(inputs.iloc[test], targets.iloc[test], verbose=0)\n",
    "        nn_acc_per_fold.append(nn_scores[1] * 100)\n",
    "\n",
    "        print(f'NN: Score for fold {fold_no}: {cv_nn_model.metrics_names[1]} of {nn_scores[1]*100}%; {cv_nn_model.metrics_names[0]} of {nn_scores[0]}')\n",
    "\n",
    "\n",
    "\n",
    "        # -- Logistic regressor --\n",
    "        #Fit logistic regressor on train split\n",
    "        lr_model = LogisticRegression(solver='liblinear')#, random_state=0)\n",
    "        lr_model.fit(inputs.iloc[train], targets.iloc[train])\n",
    "\n",
    "        #Get accuracy\n",
    "        output_lr = lr_model.predict(inputs.iloc[test])\n",
    "        lr_scores = accuracy_score(targets.iloc[test], output_lr)\n",
    "        lr_acc_per_fold.append(lr_scores*100)\n",
    "\n",
    "        print(f'LR: Score for fold {fold_no}: binary_accuracy of {lr_scores*100}%')\n",
    "\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "        \n",
    "    # == Provide average scores ==\n",
    "    #Write NN to file\n",
    "    printResults(\"NeuralNetwork\", nn_acc_per_fold, train_inf, test_inf)\n",
    "    \n",
    "    #Write LR to file\n",
    "    printResults(\"LinearLogisticRegression\", lr_acc_per_fold, train_inf,test_inf)\n",
    "    \n",
    "    #CorrectedResampledPairedT_Test(nn_acc_per_fold, lr_acc_per_fold, fold_info)\n",
    "    \n",
    "    return nn_acc_per_fold, lr_acc_per_fold, train_inf, test_inf, fold_info\n",
    "  \n",
    "nn_acc, lr_acc, trains, tests, f_inf = ten_ten_fold_cross_val()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f04efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(542, 5)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Length of train: 487 \t length of test: 55\n",
      "(487, 5)\n",
      "NN: Score for fold 1: binary_accuracy of 83.63636136054993%; loss of 0.2975699305534363\n",
      "LR: Score for fold 1: binary_accuracy of 85.45454545454545%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Length of train: 487 \t length of test: 55\n",
      "(487, 5)\n",
      "NN: Score for fold 2: binary_accuracy of 83.63636136054993%; loss of 0.38385334610939026\n",
      "LR: Score for fold 2: binary_accuracy of 81.81818181818183%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 3: binary_accuracy of 90.74074029922485%; loss of 0.31074997782707214\n",
      "LR: Score for fold 3: binary_accuracy of 83.33333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 4: binary_accuracy of 83.33333134651184%; loss of 0.45742911100387573\n",
      "LR: Score for fold 4: binary_accuracy of 79.62962962962963%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 5: binary_accuracy of 100.0%; loss of 0.1142718717455864\n",
      "LR: Score for fold 5: binary_accuracy of 94.44444444444444%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 6: binary_accuracy of 79.62962985038757%; loss of 0.30003756284713745\n",
      "LR: Score for fold 6: binary_accuracy of 81.48148148148148%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 7: binary_accuracy of 90.74074029922485%; loss of 0.18628786504268646\n",
      "LR: Score for fold 7: binary_accuracy of 87.03703703703704%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 8: binary_accuracy of 90.74074029922485%; loss of 0.36545008420944214\n",
      "LR: Score for fold 8: binary_accuracy of 83.33333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 9: binary_accuracy of 83.33333134651184%; loss of 0.2798961400985718\n",
      "LR: Score for fold 9: binary_accuracy of 85.18518518518519%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 10: binary_accuracy of 81.4814805984497%; loss of 0.3476424813270569\n",
      "LR: Score for fold 10: binary_accuracy of 75.92592592592592%\n",
      "(542, 5)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 11 ...\n",
      "Length of train: 487 \t length of test: 55\n",
      "(487, 5)\n",
      "NN: Score for fold 11: binary_accuracy of 87.27272748947144%; loss of 0.34071800112724304\n",
      "LR: Score for fold 11: binary_accuracy of 87.27272727272727%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 12 ...\n",
      "Length of train: 487 \t length of test: 55\n",
      "(487, 5)\n",
      "NN: Score for fold 12: binary_accuracy of 92.72727370262146%; loss of 0.26701128482818604\n",
      "LR: Score for fold 12: binary_accuracy of 83.63636363636363%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 13 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 13: binary_accuracy of 87.03703880310059%; loss of 0.31987836956977844\n",
      "LR: Score for fold 13: binary_accuracy of 79.62962962962963%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 14 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 14: binary_accuracy of 83.33333134651184%; loss of 0.6964588761329651\n",
      "LR: Score for fold 14: binary_accuracy of 83.33333333333334%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 15 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 15: binary_accuracy of 88.88888955116272%; loss of 0.2796153724193573\n",
      "LR: Score for fold 15: binary_accuracy of 88.88888888888889%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 16 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 16: binary_accuracy of 83.33333134651184%; loss of 0.38437002897262573\n",
      "LR: Score for fold 16: binary_accuracy of 87.03703703703704%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 17 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n",
      "NN: Score for fold 17: binary_accuracy of 85.18518805503845%; loss of 0.25843921303749084\n",
      "LR: Score for fold 17: binary_accuracy of 87.03703703703704%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 18 ...\n",
      "Length of train: 488 \t length of test: 54\n",
      "(488, 5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6d471c4fad5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnn_acc_per_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_acc_per_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_inf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_inf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[0mnn_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrains\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtests\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mten_ten_fold_cross_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-6d471c4fad5d>\u001b[0m in \u001b[0;36mten_ten_fold_cross_val\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m#Train model using train split.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_nn_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# -- Test --\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-aacb7f874b2f>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, inputs, targets)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_valid\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloud_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1261\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[0;32m   1264\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1529\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1531\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1532\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1193\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1195\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    724\u001b[0m             \u001b[1;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m             \"not be specified.\")\n\u001b[1;32m--> 726\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    749\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 751\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\.conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3235\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3236\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 3237\u001b[1;33m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[0;32m   3238\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3239\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# #based on: \n",
    "# # https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-keras.md\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def ten_ten_fold_cross_val():\n",
    "#     # Combine all of training data for cross_validation\n",
    "#     fold_info = []\n",
    "#     train_inf = []\n",
    "#     test_inf = []\n",
    "    \n",
    "#     nn_acc_per_fold= []    \n",
    "#     lr_acc_per_fold= []\n",
    "\n",
    "#     # K-fold Cross Validation model evaluation\n",
    "#     fold_no = 1\n",
    "#     for i in range(10):\n",
    "        \n",
    "#         inputs = train_data.drop(columns=\"Success\")\n",
    "#         targets = train_data[\"Success\"]\n",
    "        \n",
    "#         print(inputs.shape)\n",
    "        \n",
    "#         kfold = KFold(n_splits=10, shuffle=True)\n",
    "        \n",
    "#         for train, test in kfold.split(inputs, targets):\n",
    "#             print(f'------------------------------------------------------------------------\\nTraining for fold {fold_no} ...')\n",
    "#             #Store train test split\n",
    "#             #fold_info.append([train,test])\n",
    "#             train_inf.append(train)\n",
    "#             test_inf.append(test)\n",
    "#             print(f'Length of train: {len(train)} \\t length of test: {len(test)}')\n",
    "            \n",
    "#             # -- Neural Network --\n",
    "#             tf.keras.backend.clear_session()\n",
    "#             cv_nn_model = get_model(inputs.shape[1])\n",
    "\n",
    "#             #Train model using train split.\n",
    "#             train_model(cv_nn_model, inputs.iloc[train], targets.iloc[train])\n",
    "\n",
    "#             # -- Test --\n",
    "#             nn_scores = cv_nn_model.evaluate(inputs.iloc[test], targets.iloc[test], verbose=0)\n",
    "#             nn_acc_per_fold.append(nn_scores[1] * 100)\n",
    "            \n",
    "#             print(f'NN: Score for fold {fold_no}: {cv_nn_model.metrics_names[1]} of {nn_scores[1]*100}%; {cv_nn_model.metrics_names[0]} of {nn_scores[0]}')\n",
    "\n",
    "\n",
    "\n",
    "#             # -- Logistic regressor --\n",
    "#             #Fit logistic regressor on train split\n",
    "#             lr_model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "#             lr_model.fit(inputs.iloc[train], targets.iloc[train])\n",
    "\n",
    "#             #Get accuracy\n",
    "#             output_lr = lr_model.predict(inputs.iloc[test])\n",
    "#             lr_scores = accuracy_score(targets.iloc[test], output_lr)\n",
    "#             lr_acc_per_fold.append(lr_scores*100)\n",
    "            \n",
    "#             print(f'LR: Score for fold {fold_no}: binary_accuracy of {lr_scores*100}%')\n",
    "\n",
    "#             # Increase fold number\n",
    "#             fold_no = fold_no + 1\n",
    "\n",
    "        \n",
    "#     # == Provide average scores ==\n",
    "#     #Write NN to file\n",
    "#     printResults(\"NeuralNetwork\", nn_acc_per_fold, train_inf, test_inf)\n",
    "    \n",
    "#     #Write LR to file\n",
    "#     printResults(\"LinearLogisticRegression\", lr_acc_per_fold, train_inf,test_inf)\n",
    "    \n",
    "#     #CorrectedResampledPairedT_Test(nn_acc_per_fold, lr_acc_per_fold, fold_info)\n",
    "    \n",
    "#     return nn_acc_per_fold, lr_acc_per_fold, train_inf, test_inf\n",
    "  \n",
    "# nn_acc, lr_acc, trains, tests = ten_ten_fold_cross_val()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c50928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(len(fold_inf))\n",
    "# tests = []\n",
    "# for i in range(len(fold_inf)):\n",
    "#    tests.append(fold_inf[i][0])\n",
    "#    #print(fold_inf[i][0])\n",
    "    \n",
    "# tests = np.array(fold_inf, dtype=int, copy=True)\n",
    "# print(tests)\n",
    "# #print(fold_inf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b739431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_len =[]\n",
    "tests_len = []\n",
    "for i in range(len(trains)):\n",
    "    trains_len.append(len(trains[i]))\n",
    "    tests_len.append(len(tests[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb744d37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1:487.8 n2:54.2\n",
      "1.9665526132386482\n",
      "0.052034673938422094\n",
      "T 0.973982663030789\n"
     ]
    }
   ],
   "source": [
    "# print(fold_inf)\n",
    "\n",
    "# print(trains.shape)\n",
    "# print(tests)\n",
    "\n",
    "\n",
    "CorrectedResampledPairedT_Test(nn_acc, lr_acc, trains_len, tests_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be226a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(403, 5)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "(362, 5)\n",
      "NN: Score for fold 1: binary_accuracy of 75.60975551605225%; loss of 0.5046401619911194\n",
      "LR: Score for fold 1: binary_accuracy of 75.60975609756098%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "(362, 5)\n",
      "NN: Score for fold 2: binary_accuracy of 82.92682766914368%; loss of 0.3634245991706848\n",
      "LR: Score for fold 2: binary_accuracy of 85.36585365853658%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "(362, 5)\n",
      "NN: Score for fold 3: binary_accuracy of 87.80487775802612%; loss of 0.3177565038204193\n",
      "LR: Score for fold 3: binary_accuracy of 80.48780487804879%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "(363, 5)\n",
      "NN: Score for fold 4: binary_accuracy of 85.00000238418579%; loss of 0.29204481840133667\n",
      "LR: Score for fold 4: binary_accuracy of 82.5%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "(363, 5)\n",
      "NN: Score for fold 5: binary_accuracy of 87.5%; loss of 0.3859739899635315\n",
      "LR: Score for fold 5: binary_accuracy of 75.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "(363, 5)\n",
      "NN: Score for fold 6: binary_accuracy of 80.0000011920929%; loss of 0.4987662732601166\n",
      "LR: Score for fold 6: binary_accuracy of 80.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "(363, 5)\n",
      "NN: Score for fold 7: binary_accuracy of 92.5000011920929%; loss of 0.3138362765312195\n",
      "LR: Score for fold 7: binary_accuracy of 90.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "(363, 5)\n",
      "NN: Score for fold 8: binary_accuracy of 87.5%; loss of 0.36754006147384644\n",
      "LR: Score for fold 8: binary_accuracy of 80.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "(363, 5)\n",
      "NN: Score for fold 9: binary_accuracy of 77.49999761581421%; loss of 0.3788076937198639\n",
      "LR: Score for fold 9: binary_accuracy of 75.0%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "(363, 5)\n",
      "NN: Score for fold 10: binary_accuracy of 87.5%; loss of 0.2637586295604706\n",
      "LR: Score for fold 10: binary_accuracy of 90.0%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "NeuralNetwork:\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5046401619911194 - Accuracy: 75.60975551605225%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.3634245991706848 - Accuracy: 82.92682766914368%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3177565038204193 - Accuracy: 87.80487775802612%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.29204481840133667 - Accuracy: 85.00000238418579%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3859739899635315 - Accuracy: 87.5%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.4987662732601166 - Accuracy: 80.0000011920929%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.3138362765312195 - Accuracy: 92.5000011920929%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.36754006147384644 - Accuracy: 87.5%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.3788076937198639 - Accuracy: 77.49999761581421%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.2637586295604706 - Accuracy: 87.5%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds\n",
      "nNeuralNetwork:\n",
      "> Accuracy: 84.38414633274078 (+- 5.027915657086029)\n",
      "> Loss: 0.3686549007892609\n",
      "------------------------------------------------------------------------\n",
      "LogisticRegression\n",
      "Score per fold:\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Accuracy: 0.7560975609756098%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Accuracy: 0.8536585365853658%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Accuracy: 0.8048780487804879%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Accuracy: 0.825%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Accuracy: 0.75%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Accuracy: 0.8%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Accuracy: 0.9%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Accuracy: 0.8%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Accuracy: 0.75%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Accuracy: 0.9%\n",
      "------------------------------------------------------------------------\\Average scores for all folds:\n",
      "> Accuracy: 0.8139634146341465 (+- 0.05339071883598052)\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# def NN_train_and_results(model, inputs, targets, train, test):\n",
    "#     return\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "# #based on: \n",
    "# # https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-keras.md\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def ten_ten_fold_cross_val():\n",
    "#     # Combine all of training data for cross_validation\n",
    "#     inputs = train_data.drop(columns=\"Success\")\n",
    "#     targets = train_data[\"Success\"]\n",
    "#     print(inputs.shape)\n",
    "    \n",
    "#     kfold = KFold(n_splits=10, shuffle=True)\n",
    "    \n",
    "#     nn_acc_per_fold= []\n",
    "#     nn_loss_per_fold= []\n",
    "    \n",
    "#     lr_acc_per_fold= []\n",
    "\n",
    "#     # K-fold Cross Validation model evaluation\n",
    "#     fold_no = 1\n",
    "#     for train, test in kfold.split(inputs, targets):\n",
    "#         print(f'------------------------------------------------------------------------\\nTraining for fold {fold_no} ...')\n",
    "    \n",
    "#         # -- Neural Network\n",
    "#         tf.keras.backend.clear_session()\n",
    "#         cv_nn_model = get_model(inputs.shape[1])\n",
    "        \n",
    "#         #Train model using train split.\n",
    "#         train_model(cv_nn_model, inputs.iloc[train], targets.iloc[train])\n",
    "        \n",
    "#         # -- Test --\n",
    "#         nn_scores = cv_nn_model.evaluate(inputs.iloc[test], targets.iloc[test], verbose=0)\n",
    "#         print(f'NN: Score for fold {fold_no}: {cv_nn_model.metrics_names[1]} of {nn_scores[1]*100}%; {cv_nn_model.metrics_names[0]} of {nn_scores[0]}')\n",
    "#         nn_acc_per_fold.append(nn_scores[1] * 100)\n",
    "#         nn_loss_per_fold.append(nn_scores[0])\n",
    "        \n",
    "    \n",
    "#         # -- Logistic regressor --\n",
    "        \n",
    "#         #Fit logistic regressor on train split\n",
    "#         lr_model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "#         lr_model.fit(inputs.iloc[train], targets.iloc[train])\n",
    "        \n",
    "#         #Get accuracy\n",
    "#         output_lr = lr_model.predict(inputs.iloc[test])\n",
    "#         lr_scores = accuracy_score(targets.iloc[test], output_lr)\n",
    "        \n",
    "#         print(f'LR: Score for fold {fold_no}: binary_accuracy of {lr_scores*100}%')\n",
    "#         lr_acc_per_fold.append(lr_scores)\n",
    "\n",
    "        \n",
    "#         # Increase fold number\n",
    "#         fold_no = fold_no + 1\n",
    "\n",
    "        \n",
    "#     # == Provide average scores ==\n",
    "#     #Write NN to file\n",
    "#     nn_output_file = open('CrossValidationResults\\\\NeuralNetwork.csv', \"w\")\n",
    "#     nn_output_file.write('Fold,Accuracy,loss'+'\\n')\n",
    "#     print('------------------------------------------------------------------------\\nScore per fold\\nNeuralNetwork:')\n",
    "#     for i in range(0, len(nn_acc_per_fold)):\n",
    "#         print('------------------------------------------------------------------------')\n",
    "#         print(f'> Fold {i+1} - Loss: {nn_loss_per_fold[i]} - Accuracy: {nn_acc_per_fold[i]}%')\n",
    "#         nn_output_file.write(f'{i+1},{nn_acc_per_fold[i]},{nn_loss_per_fold[i]}'+'\\n')\n",
    "        \n",
    "#     nn_output_file.write(f'{np.mean(nn_acc_per_fold)},{np.std(nn_acc_per_fold)},{np.mean(nn_loss_per_fold)}'+'\\n')\n",
    "#     nn_output_file.close()\n",
    "        \n",
    "#     print('------------------------------------------------------------------------\\nAverage scores for all folds\\nnNeuralNetwork:')\n",
    "#     print(f'> Accuracy: {np.mean(nn_acc_per_fold)} (+- {np.std(nn_acc_per_fold)})')\n",
    "#     print(f'> Loss: {np.mean(nn_loss_per_fold)}')\n",
    "    \n",
    "    \n",
    "#     #Write LR to file\n",
    "#     lr_output_file = open('CrossValidationResults\\LinearLogisticRegression.csv', \"w\")\n",
    "#     lr_output_file.write('Fold,Accuracy'+'\\n')\n",
    "#     print('------------------------------------------------------------------------\\nLogisticRegression\\nScore per fold:')\n",
    "#     for i in range(0, len(lr_acc_per_fold)):\n",
    "#         print('------------------------------------------------------------------------')\n",
    "#         print(f'> Fold {i+1} - Accuracy: {lr_acc_per_fold[i]}%')\n",
    "#         lr_output_file.write(f'{i+1},{lr_acc_per_fold[i]}'+'\\n')\n",
    "\n",
    "#     print('------------------------------------------------------------------------\\Average scores for all folds:')\n",
    "#     print(f'> Accuracy: {np.mean(lr_acc_per_fold)} (+- {np.std(lr_acc_per_fold)})')\n",
    "#     print('------------------------------------------------------------------------')\n",
    "    \n",
    "#     lr_output_file.write(f'{np.mean(lr_acc_per_fold)},{np.std(lr_acc_per_fold)}'+'\\n')\n",
    "#     lr_output_file.close()\n",
    "    \n",
    "    \n",
    "#     #Compute the difference between the results\n",
    "#     #diff = [y - x for y, x in zip(RFC_score, SVM_score)]\n",
    "#     diff = [y - x for y, x in zip(nn_acc_per_fold, lr_acc_per_fold)]\n",
    "    \n",
    "#     #Comopute the mean of differences\n",
    "#     d_bar = np.mean(diff)\n",
    "    \n",
    "#     #compute the variance of differences\n",
    "#     sigma2 = np.var(diff)\n",
    "    \n",
    "#     #compute the number of data points used for training \n",
    "#     #n1 = len(y_train)\n",
    "#     n1 = len(targets.iloc[train])\n",
    "    \n",
    "#     #compute the number of data points used for testing \n",
    "#     #n2 = len(y_test)\n",
    "#     n2 = len(targets.iloc[test])\n",
    "    \n",
    "#     #compute the total number of data points\n",
    "#     #n = len(y)\n",
    "#     n = len(targets)\n",
    "    \n",
    "#     #compute the modified variance\n",
    "#     sigma2_mod = sigma2 * (1/n + n2/n1)\n",
    "    \n",
    "#     #compute the t_static\n",
    "#     t_static =  d_bar / np.sqrt(sigma2_mod)\n",
    "    \n",
    "#     from scipy.stats import t\n",
    "    \n",
    "#     #Compute p-value and plot the results \n",
    "#     Pvalue = ((1 - t.cdf(t_static, n-1))*200)\n",
    "    \n",
    "#     Pvalue\n",
    "\n",
    "  \n",
    "# ten_ten_fold_cross_val()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d58ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "#based on: \n",
    "# https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-keras.md\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def ten_ten_fold_cross_val():\n",
    "    # Combine all of training data for cross_validation\n",
    "    fold_info = []\n",
    "    train_inf = []\n",
    "    test_inf = []\n",
    "    \n",
    "    nn_acc_per_fold= []    \n",
    "    lr_acc_per_fold= []\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    fold_no = 1\n",
    "    \n",
    "    inputs = train_data.drop(columns=\"Success\")\n",
    "    targets = train_data[\"Success\"]\n",
    "\n",
    "    print(inputs.shape)\n",
    "\n",
    "    #kfold = KFold(n_splits=10, shuffle=True)\n",
    "    kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=10)#,random_state=0)\n",
    "        \n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "        print(f'------------------------------------------------------------------------\\nTraining for fold {fold_no} ...')\n",
    "        #Store train test split\n",
    "        \n",
    "        oversample = SMOTE()\n",
    "        train_in, train_targ = oversample.fit_resample(inputs.iloc[train], targets.iloc[train])\n",
    "        \n",
    "        fold_info.append([train,test])\n",
    "        train_inf.append(train)\n",
    "        test_inf.append(test)\n",
    "        print(f'Length of train: {len(train)} \\t length of test: {len(test)}')\n",
    "\n",
    "        # -- Neural Network --\n",
    "        tf.keras.backend.clear_session()\n",
    "        cv_nn_model = get_model(inputs.shape[1])\n",
    "\n",
    "        #Train model using train split.\n",
    "        train_model(cv_nn_model, inputs.iloc[train], targets.iloc[train])\n",
    "\n",
    "        # -- Test --\n",
    "        nn_scores = cv_nn_model.evaluate(inputs.iloc[test], targets.iloc[test], verbose=0)\n",
    "        nn_acc_per_fold.append(nn_scores[1] * 100)\n",
    "\n",
    "        print(f'NN: Score for fold {fold_no}: {cv_nn_model.metrics_names[1]} of {nn_scores[1]*100}%; {cv_nn_model.metrics_names[0]} of {nn_scores[0]}')\n",
    "\n",
    "\n",
    "\n",
    "        # -- Logistic regressor --\n",
    "        #Fit logistic regressor on train split\n",
    "        lr_model = LogisticRegression(solver='liblinear')#, random_state=0)\n",
    "        lr_model.fit(inputs.iloc[train], targets.iloc[train])\n",
    "\n",
    "        #Get accuracy\n",
    "        output_lr = lr_model.predict(inputs.iloc[test])\n",
    "        lr_scores = accuracy_score(targets.iloc[test], output_lr)\n",
    "        lr_acc_per_fold.append(lr_scores*100)\n",
    "\n",
    "        print(f'LR: Score for fold {fold_no}: binary_accuracy of {lr_scores*100}%')\n",
    "\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "        \n",
    "    # == Provide average scores ==\n",
    "    #Write NN to file\n",
    "    printResults(\"NeuralNetwork\", nn_acc_per_fold, train_inf, test_inf)\n",
    "    \n",
    "    #Write LR to file\n",
    "    printResults(\"LinearLogisticRegression\", lr_acc_per_fold, train_inf,test_inf)\n",
    "    \n",
    "    #CorrectedResampledPairedT_Test(nn_acc_per_fold, lr_acc_per_fold, fold_info)\n",
    "    \n",
    "    return nn_acc_per_fold, lr_acc_per_fold, train_inf, test_inf, fold_info\n",
    "  \n",
    "nn_acc, lr_acc, trains, tests, f_inf = ten_ten_fold_cross_val()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
